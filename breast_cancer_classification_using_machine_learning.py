# -*- coding: utf-8 -*-
"""Breast-Cancer-Classification-Using-Machine-Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g_crXNoj6u90KZVtlvG_FBXKFuiMnvju

# **Breast Cancer Classification Using Machine Learning**

Link to dataset : https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic

1. **Dataset Information** :
Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/

2. **Input Variables** :     
  Ten real-valued features are computed for each cell nucleus:

	a) radius (mean of distances from center to points on the perimeter)

	b) texture (standard deviation of gray-scale values)

	c) perimeter

	d) area

	e) smoothness (local variation in radius lengths)

	f) compactness (perimeter^2 / area - 1.0)

	g) concavity (severity of concave portions of the contour)

	h) concave points (number of concave portions of the contour)

	i) symmetry

	j) fractal dimension ("coastline approximation" - 1)



3. **Ouput Variable** :    
Diagnosis (M = malignant, B = benign)

4. **Models Used** :    
    1. Random Forest Classification
    2. Logistic Regression
    3. Xgboost Classification


Github Repo : https://github.com/rajat-kumar-thakur/Breast-Cancer-Classification-Using-Machine-Learning
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import time
from subprocess import check_output
import warnings
warnings.filterwarnings('ignore')
from scipy import stats

"""## **1. Dataset Collection and Visualization**"""

data = pd.read_csv('/content/drive/MyDrive/Breast-Cancer-Classification-Using-Machine-Learning/data.csv')

"""Lets look at all the features of our dataset."""

data.head()

col = data.columns
print(col)

"""We will drop these three columns for the following reason:
1. There is an id that cannot be used for classificaiton.
2. Diagnosis is our class label.
3. Unnamed: 32 feature includes NaN so we do not need it.
"""

y = data.diagnosis
list = ['Unnamed: 32','id','diagnosis']
x = data.drop(list,axis = 1 )
x.head()

"""Let us take a look at our output and input variables."""

ax = sns.countplot(y,label="Count")
B, M = y.value_counts()
print('Number of Benign: ',B)
print('Number of Malignant : ',M)

x.describe()

"""### Visualizations

#### 1. Box Plot
"""

# first ten features
data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())
data = pd.concat([y, data_n_2.iloc[:, 0:10]], axis=1)
data = pd.melt(data, id_vars="diagnosis",
               var_name="features",
               value_name='value')

plt.figure(figsize=(10, 10))
sns.boxplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)
plt.show()

# next ten features (features 11-20)
data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())
data = pd.concat([y, data_n_2.iloc[:, 10:20]], axis=1)
data = pd.melt(data, id_vars="diagnosis",
               var_name="features",
               value_name='value')

plt.figure(figsize=(10, 10))
sns.boxplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)
plt.title("Box Plot for Features 11-20")
plt.show()

# next ten features (features 21-30)
data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())
data = pd.concat([y, data_n_2.iloc[:, 20:30]], axis=1)
data = pd.melt(data, id_vars="diagnosis",
               var_name="features",
               value_name='value')
plt.figure(figsize=(10, 10))
sns.boxplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)
plt.title("Box Plot for Features 21-30")
plt.show()

"""In texture_mean feature, median of the Malignant and Benign looks like separated so it can be good for classification. However, in fractal_dimension_mean feature, median of the Malignant and Benign does not looks like separated so it does not gives good information for classification.

#### 2. Joint Plot

In order to compare two features deeper, lets use joint plot. Look at this in joint plot below, it is really correlated. Pearson value is correlation value and 1 is the highest. Therefore, 0.855 is looks enough to say that they are correlated.
"""

g = sns.jointplot(x='concavity_worst', y='concave points_worst', data=x, kind="reg", color="red")

r = x['concavity_worst'].corr(x['concave points_worst'])
plt.subplots_adjust(top=0.9)
plt.suptitle(f'Pearson r: {r:.3f}', fontsize=12)

plt.show()

"""#### 3. Violin plots

The violin plot is particularly useful for this dataset because it combines elements of a box plot with a kernel density plot, providing a more comprehensive view of your data's distribution than simpler visualizations.
"""

data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)

"""#### 4. Swarm Plot

In these three plots which feature looks like more clear in terms of classification. In my opinion area_worst in last swarm plot looks like malignant and benign are seprated not totaly but mostly. Hovewer, smoothness_se in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while using this feature.
"""

sns.set(style="whitegrid", palette="muted")
data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
tic = time.time()
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)

plt.xticks(rotation=90)

data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)

data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)
toc = time.time()
plt.xticks(rotation=90)
print("swarm plot time: ", toc-tic ," s")

"""#### 5. Heatmap (Correlation Matrix)

We use heatmap to observe all correlation between features.
"""

f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)

"""## **2. Data Preprocessing**

### 2.1 Data Cleaning

We dropped three columns for the following reasons:
1. There is an id that cannot be used for classificaiton.
2. Diagnosis is our class label.
3. Unnamed: 32 feature includes NaN so we do not need it.



```
y = data.diagnosis
list = ['Unnamed: 32','id','diagnosis']
x = data.drop(list,axis = 1 )
x.head()
```
* These columns are already dropped from the dataset.

### 2.2 Feature Creation

I added two new features to the dataset i.e volume and surface area and their respective columns.

These calculated features is a form of feature engineering that leverages domain knowledge to improve analytical capabilities, potentially revealing patterns that wouldn't be immediately apparent from the raw measurements alone.
"""

x_processed = x.copy()

x_processed['volume_mean'] = (4/3) * np.pi * (x_processed['radius_mean'] ** 3)
x_processed['volume_worst'] = (4/3) * np.pi * (x_processed['radius_worst'] ** 3)

x_processed['surface_area_mean'] = 4 * np.pi * (x_processed['radius_mean'] ** 2)
x_processed['surface_area_to_volume_ratio'] = x_processed['surface_area_mean'] / x_processed['volume_mean']

x_processed.head()

x.head()

"""### 2.3 Visualizations"""

plt.figure(figsize=(20, 10))
plt.suptitle("Initial vs Preprocessed Data", fontsize=18)

# Plot 1: radius_mean
plt.subplot(2, 2, 1)
sns.histplot(x['radius_mean'], bins=30, kde=True, color='blue')
plt.title('Initial Feature: radius_mean')

# Plot 2: volume_mean
plt.subplot(2, 2, 2)
sns.histplot(x_processed['volume_mean'], bins=30, kde=True, color='green')
plt.title('New Feature: volume_mean')

# Plot 3: surface_area_mean
plt.subplot(2, 2, 3)
sns.histplot(x_processed['surface_area_mean'], bins=30, kde=True, color='orange')
plt.title('New Feature: surface_area_mean')

# Plot 4: surface_area_to_volume_ratio
plt.subplot(2, 2, 4)
sns.histplot(x_processed['surface_area_to_volume_ratio'], bins=30, kde=True, color='red')
plt.title('New Feature: surface_area_to_volume_ratio')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""**Graph Insights:**

1. The volume_mean histogram tends to be more skewed to the right. The transformation using the cubic function means that even small differences in radius lead to much larger differences in volume.

2. The distribution of surface_area_mean is calculated using the square of the radius. Its histogram usually shows a distribution that is between the linear scale of the radius and the cubic scale of the volume.

3. The histogram for surface_area_to_volume_ratio appears differentlyâ€”its distribution may be somewhat left-skewed, reflecting how the ratio decreases as the cell radius increases.

4. The transformation to volume and surface area features allows models or analyses to capture nonlinear effects that the linear radius cannot.

## **3. Data Transformation and Statistical Analysis**

### 3.1 Feature selection with correlation and random forest classification

As it can be seen in map heat figure radius_mean, perimeter_mean and area_mean are correlated with each other so we will use only area_mean. So lets find other correlated features and look accuracy with random forest classifier.

Compactness_mean, concavity_mean and concave points_mean are correlated with each other.Therefore I only choose concavity_mean. Apart from these, radius_se, perimeter_se and area_se are correlated and I only use area_se. radius_worst, perimeter_worst and area_worst are correlated so I use area_worst. Compactness_worst, concavity_worst and concave points_worst so I use concavity_worst. Compactness_se, concavity_se and concave points_se so I use concavity_se. texture_mean and texture_worst are correlated and I use texture_mean. area_worst and area_mean are correlated, I use area_mean.
"""

x = x_processed.copy()

drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']
x_1 = x.drop(drop_list1,axis = 1 )
x_1.head()

f,ax = plt.subplots(figsize=(14, 14))
sns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score,confusion_matrix
from sklearn.metrics import accuracy_score

x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)

clf_rf = RandomForestClassifier(random_state=43)
clr_rf = clf_rf.fit(x_train,y_train)

ac = accuracy_score(y_test,clf_rf.predict(x_test))
print('Accuracy is: ',ac)
cm = confusion_matrix(y_test,clf_rf.predict(x_test))
sns.heatmap(cm,annot=True,fmt="d")

"""Accuracy is almost 97% and as it can be seen in confusion matrix, we made few wrong prediction.

In random forest classification method there is a featureimportances attributes that is the feature importances (the higher, the more important the feature).
"""

clf_rf_5 = RandomForestClassifier()
clr_rf_5 = clf_rf_5.fit(x_train,y_train)
importances = clr_rf_5.feature_importances_
std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

print("Feature ranking:")

for f in range(x_train.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))


plt.figure(1, figsize=(14, 13))
plt.title("Feature importances")
plt.bar(range(x_train.shape[1]), importances[indices],
       color="g", yerr=std[indices], align="center")
plt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)
plt.xlim([-1, x_train.shape[1]])
plt.show()

"""As you can seen in plot above, after 7 best features importance of features decrease. Therefore we can focus these 7 features.

### 3.2 Feature Extraction with PCA

We will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.
"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
#normalization
x_train_N = (x_train-x_train.mean())/(x_train.max()-x_train.min())
x_test_N = (x_test-x_test.mean())/(x_test.max()-x_test.min())

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(x_train_N)

plt.figure(1, figsize=(14, 13))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_ratio_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_ratio_')

"""**According to variance ratio, 3 components can be chosen.**

### 3.3 Hypothesis Testing

I want to learn that are radius mean and area mean related with each other? My null hypothesis is that "relationship between radius mean and area mean is zero in tumor population'.


Now we need to refute this null hypothesis in order to demonstrate that radius mean and area mean are related. (actually we know it from our previous experiences)


lets find p-value (probability value)
"""

statistic, p_value = stats.ttest_rel(x.radius_mean,x.area_mean)
print('p-value: ',p_value)

"""P values is almost zero so we can reject null hypothesis.

## **4. Model Implementation**

### 4.1 Random Forest Classification

The Random Forest Classifier by default does not apply any specific strategy to address class imbalance. It builds multiple decision trees using bootstrap samples, and each tree is built without being explicitly told to balance class weights. This means that if one class appears more frequently than another, the model may favor the majority class in its predictions.
"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)

# Train a Random Forest Classifier (default n_estimators=10)
clf_rf = RandomForestClassifier(random_state=43)
clf_rf.fit(x_train, y_train)

accuracy_score_rf = accuracy_score(y_test, clf_rf.predict(x_test))
print('Accuracy is:', accuracy_score_rf)

# Compute confusion matrix
cm_rf = confusion_matrix(y_test, clf_rf.predict(x_test))
plt.figure(figsize=(8,6))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Generate classification report
report = classification_report(y_test, clf_rf.predict(x_test), output_dict=True)
report_df = pd.DataFrame(report).transpose()
print("\nClassification Report:")
print(report_df)

"""### 4.2 Logistic Regression

Logistic Regression is a popular choice for classification tasks because it models the probability of an instance belonging to a particular class using the logistic (sigmoid) function. This makes it especially useful for binary classification problems.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Split the dataset: 70% training, 30% testing
x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)

lr_model = LogisticRegression(solver='liblinear', random_state=42)
lr_model.fit(x_train, y_train)

predictions = lr_model.predict(x_test)

# Calculate Accuracy
accuracy_score_lr = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy_score_lr)

# Plot Confusion Matrix
cm_lr = confusion_matrix(y_test, predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_lr, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Generate Classification Report and display it as a table
report = classification_report(y_test, predictions, output_dict=True)
report_df = pd.DataFrame(report).transpose()
print("\nClassification Report:")
print(report_df)

"""By default, Logistic Regression does not adjust for class imbalance. However, if there is a class imbalance issue, we specified the parameter class_weight='balanced' when initializing the model. This parameter automatically adjusts the weights inversely proportional to class frequencies to mitigate bias towards the majority class.

### 4.3 Xgboost Classification

XGBoost is known for its high performance and speed, especially on structured or tabular datasets. It uses gradient boosting on decision trees to capture complex interactions in the data efficiently.
"""

from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Split the dataset: 70% training, 30% testing
x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target variable
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Create and train the XGBoost model
xgb_model = XGBClassifier(random_state=42, n_estimators=100, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(x_train, y_train_encoded) # Use encoded target variable for training

# Predictions on the test data (using encoded test data)
predictions = xgb_model.predict(x_test)

# Decode predictions back to original labels for evaluation
decoded_predictions = label_encoder.inverse_transform(predictions)

# Calculate accuracy (using decoded predictions)
accuracy_score_xg = accuracy_score(y_test, decoded_predictions)
print("Accuracy:", accuracy_score_xg)

# Plot Confusion Matrix (using decoded predictions)
cm_xg = confusion_matrix(y_test, decoded_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xg, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix - XGBoost Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Generate Classification Report (using decoded predictions) and display as table
report = classification_report(y_test, decoded_predictions, output_dict=True)
report_df = pd.DataFrame(report).transpose()
print("\nClassification Report:")
print(report_df)

"""XGBoost does not automatically solve class imbalance problems. However, you can address imbalance by adjusting the parameter scale_pos_weight (commonly used in binary classification) to assign a higher weight to the minority class.

## **5. Results and Comparisons**
"""

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(18, 12))
fig.suptitle("Model Performance Comparisons", fontsize=20)

# 1. Bar chart for Accuracy Comparison with a zoomed y-axis
models = ['Random Forest', 'Logistic Regression', 'XGBoost']
accuracies = [accuracy_score_rf, accuracy_score_lr, accuracy_score_xg]
sns.barplot(x=models, y=accuracies, palette="viridis", ax=axes[0, 0])
axes[0, 0].set_title("Accuracy Comparison (Zoomed)")
axes[0, 0].set_ylabel("Accuracy")
y_min = min(accuracies) - 0.01
y_max = max(accuracies) + 0.01
axes[0, 0].set_ylim(y_min, y_max)

# 2. Confusion Matrix for Random Forest
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues", ax=axes[0, 1])
axes[0, 1].set_title("Random Forest Confusion Matrix")
axes[0, 1].set_xlabel("Predicted Label")
axes[0, 1].set_ylabel("True Label")

# 3. Confusion Matrix for Logistic Regression
sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Greens", ax=axes[1, 0])
axes[1, 0].set_title("Logistic Regression Confusion Matrix")
axes[1, 0].set_xlabel("Predicted Label")
axes[1, 0].set_ylabel("True Label")

# 4. Confusion Matrix for XGBoost
sns.heatmap(cm_xg, annot=True, fmt="d", cmap="Oranges", ax=axes[1, 1])
axes[1, 1].set_title("XGBoost Confusion Matrix")
axes[1, 1].set_xlabel("Predicted Label")
axes[1, 1].set_ylabel("True Label")

# Annotate each bar with its accuracy value
for i, acc in enumerate(accuracies):
    axes[0, 0].text(i, acc + 0.001, f"{acc:.3f}", ha='center', va='bottom', fontsize=12)

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""**Accuracy Comparison**
  
  Random Forest and XGBoost share the highest reported accuracy (around 0.971), slightly outperforming Logistic Regression (around 0.959). All three algorithms performed relatively well in this specific dataset.

**Confusion Matrices**

Each confusion matrix provides a deeper look into the types of errors each model makes:

1. Random Forest:
Shows a small number of false positives and false negatives.
High correct classifications in both classes, consistent with its top accuracy score.

2. Logistic Regression:
Slightly more misclassifications than Random Forest and XGBoost (noticeable in the off-diagonal cells).
Even so, it still achieves a respectable accuracy (about 95.9%).

3. XGBoost:
Confusion matrix looks similar to Random Forestâ€™s with very few off-diagonal errors.
Maintains high accuracy due to effectively classifying both classes correctly.

From these visual results alone, itâ€™s clear:

1. Random Forest and XGBoost are nearly tied for best performance in terms of accuracy and confusion matrix outcomes.

2. Logistic Regression remains competitive but slightly underperforms the other two.

3. For practical decisions, choosing between Random Forest and XGBoost may come down to other considerations like training speed, interpretability, or resource constraints, given their near-identical performance.

**Best Model Selection Criteria**
1. Precision:
Measures the proportion of correctly predicted positive instances among all instances predicted as positive.
Criterion: High precision is critical when the cost of a false positive is high (e.g., in spam detection, diagnosing disease).

2. Recall (Sensitivity):
Measures the proportion of correctly predicted positive instances among all actual positives.
Criterion: High recall is essential when missing a positive case would be costly (e.g., early cancer detection).

3. F1-Score:
The harmonic mean of precision and recall.
Criterion: F1-score provides a balance between precision and recall, particularly useful when both false positives and false negatives carry similar consequences.

**Model Complexity & Performance:**

Random Forest and XGBoost not only maintain higher overall accuracy but also deliver balanced performance in terms of precision and recall, especially by handling the nonlinear interactions more effectively than Logistic Regression.

1. Trade-Off Between Precision and Recall:

* Random Forest excels in precision for malignant cases, reducing false positives.

* XGBoost improves on recall for malignant cases, ensuring fewer cases are missed, which can be crucial in medical diagnostics.

2. Use Case Considerations:

* If reducing false positives is the priority (e.g., avoiding unnecessary interventions), Random Forest might be preferred.

* If catching as many true malignant cases as possible is critical, XGBoostâ€™s higher recall makes it attractive.

* Logistic Regression, while competitive, may serve as a good baseline but might not capture nonlinear dynamics as effectively as the tree-based models.

**Submitted By**
**Rajat Kumar Thakur (202211070)**
"""